#!/usr/bin/env python3
"""
Script para evaluar consultas con preguntas de prueba
"""
import sys
import os
import json
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.query.query_handler import QueryHandler

def main():
    print("ðŸ“Š EvaluaciÃ³n del Sistema de Consultas")
    print("=" * 50)
    
    # Consultas de prueba
    test_queries = [
        "Â¿CuÃ¡l es el demandante del expediente?",
        "Â¿QuÃ© tipo de medida se solicitÃ³?",
        "Â¿CuÃ¡l es la cuantÃ­a del embargo?",
        "Â¿En quÃ© fecha se dictÃ³ la medida?",
        "Â¿CuÃ¡les son los hechos principales del caso?",
        "Â¿QuÃ© fundamentos jurÃ­dicos se esgrimen?",
        "Â¿CuÃ¡les son las medidas cautelares solicitadas?",
        "Resume el expediente",
        "Â¿CuÃ¡l es el estado actual del proceso?",
        "Â¿QuiÃ©n es el juez del caso?"
    ]
    
    # Inicializar query handler
    query_handler = QueryHandler()
    
    results = []
    
    print(f"ðŸ§ª Evaluando {len(test_queries)} consultas...")
    
    for i, query in enumerate(test_queries, 1):
        print(f"\n[{i}/{len(test_queries)}] Procesando: {query}")
        
        try:
            result = query_handler.handle_query(query)
            
            # Evaluar calidad de respuesta
            quality_score = evaluate_response_quality(result['response'])
            
            result['quality_score'] = quality_score
            results.append(result)
            
            print(f"   âœ… Completado - Calidad: {quality_score}/5")
            
        except Exception as e:
            print(f"   âŒ Error: {e}")
            results.append({
                'query': query,
                'error': str(e),
                'quality_score': 0
            })
    
    # Calcular estadÃ­sticas
    successful = len([r for r in results if 'error' not in r])
    avg_quality = sum(r['quality_score'] for r in results) / len(results)
    
    print(f"\nðŸ“Š Resultados de EvaluaciÃ³n:")
    print(f"   - Consultas exitosas: {successful}/{len(test_queries)}")
    print(f"   - Calidad promedio: {avg_quality:.2f}/5")
    print(f"   - Tasa de Ã©xito: {(successful/len(test_queries))*100:.1f}%")
    
    # Guardar resultados
    with open("logs/query_evaluation_results.json", "w") as f:
        json.dump(results, f, indent=2, default=str)
    
    print(f"\nðŸ’¾ Resultados guardados en logs/query_evaluation_results.json")

def evaluate_response_quality(response: str) -> int:
    """Evaluar calidad de respuesta (1-5)"""
    if not response or "Error" in response:
        return 1
    
    # Criterios de calidad
    score = 0
    
    # Respuesta no vacÃ­a
    if len(response) > 10:
        score += 1
    
    # Incluye informaciÃ³n especÃ­fica
    if any(keyword in response.lower() for keyword in ['demandante', 'demandado', 'embargo', 'medida']):
        score += 1
    
    # Incluye fuente
    if 'Fuente:' in response:
        score += 1
    
    # Respuesta coherente
    if not response.startswith("No se encuentra"):
        score += 1
    
    # Respuesta completa
    if len(response) > 50:
        score += 1
    
    return score

if __name__ == "__main__":
    main() 