# 03. Implementaci√≥n del Sistema de Chunking - MVP RAG

## üéØ Objetivo
Implementar el sistema de chunking con fallback recursivo para dividir documentos legales en fragmentos manejables, preservando la estructura sem√°ntica y los metadatos.

## üìã Tareas a Ejecutar

### 1. Crear M√≥dulo de Chunking
Crear `src/chunking/document_chunker.py`:
```python
"""
M√≥dulo para chunking de documentos legales con fallback recursivo
"""
import re
import uuid
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass
from config.settings import CHUNK_SIZE, CHUNK_OVERLAP
from src.utils.logger import setup_logger

logger = setup_logger(__name__, "logs/chunking.log")

@dataclass
class Chunk:
    """Representa un chunk de documento"""
    id: str
    text: str
    position: int
    total_chunks: int
    metadata: Dict
    start_token: int
    end_token: int
    overlap_start: Optional[int] = None
    overlap_end: Optional[int] = None

class DocumentChunker:
    def __init__(self, chunk_size: int = CHUNK_SIZE, overlap: int = CHUNK_OVERLAP):
        self.chunk_size = chunk_size
        self.overlap = overlap
        self.logger = logger
        
    def _tokenize_text(self, text: str) -> List[str]:
        """Tokenizar texto en palabras/palabras"""
        # Tokenizaci√≥n simple por espacios y puntuaci√≥n
        tokens = re.findall(r'\b\w+\b', text.lower())
        return tokens
    
    def _count_tokens(self, text: str) -> int:
        """Contar tokens en un texto"""
        return len(self._tokenize_text(text))
    
    def _split_by_sentences(self, text: str) -> List[str]:
        """Dividir texto por oraciones"""
        # Patrones para detectar fin de oraci√≥n
        sentence_patterns = [
            r'[.!?]+[\s\n]*',  # Punto, exclamaci√≥n, interrogaci√≥n
            r'[.!?]+["\']+[\s\n]*',  # Con comillas
            r'\n\s*\n',  # P√°rrafos
        ]
        
        # Combinar patrones
        pattern = '|'.join(sentence_patterns)
        sentences = re.split(pattern, text)
        
        # Limpiar oraciones vac√≠as
        sentences = [s.strip() for s in sentences if s.strip()]
        return sentences
    
    def _split_by_paragraphs(self, text: str) -> List[str]:
        """Dividir texto por p√°rrafos"""
        paragraphs = re.split(r'\n\s*\n', text)
        return [p.strip() for p in paragraphs if p.strip()]
    
    def _create_chunk_with_metadata(self, 
                                  text: str, 
                                  position: int, 
                                  total_chunks: int,
                                  base_metadata: Dict,
                                  start_token: int,
                                  end_token: int) -> Chunk:
        """Crear un chunk con metadatos completos"""
        chunk_id = f"{base_metadata.get('document_id', 'doc')}_chunk_{position}"
        
        # Copiar metadatos base y a√±adir informaci√≥n del chunk
        chunk_metadata = base_metadata.copy()
        chunk_metadata.update({
            'chunk_id': chunk_id,
            'chunk_position': position,
            'total_chunks': total_chunks,
            'start_token': start_token,
            'end_token': end_token,
            'chunk_size': len(text),
            'token_count': self._count_tokens(text)
        })
        
        return Chunk(
            id=chunk_id,
            text=text,
            position=position,
            total_chunks=total_chunks,
            metadata=chunk_metadata,
            start_token=start_token,
            end_token=end_token
        )
    
    def _apply_fallback_recursive(self, text: str, max_size: int) -> List[str]:
        """Aplicar fallback recursivo para textos que exceden el tama√±o m√°ximo"""
        if self._count_tokens(text) <= max_size:
            return [text]
        
        # Intentar dividir por p√°rrafos primero
        paragraphs = self._split_by_paragraphs(text)
        if len(paragraphs) > 1:
            result = []
            for paragraph in paragraphs:
                result.extend(self._apply_fallback_recursive(paragraph, max_size))
            return result
        
        # Si no hay p√°rrafos, dividir por oraciones
        sentences = self._split_by_sentences(text)
        if len(sentences) > 1:
            result = []
            current_chunk = ""
            
            for sentence in sentences:
                test_chunk = current_chunk + " " + sentence if current_chunk else sentence
                
                if self._count_tokens(test_chunk) <= max_size:
                    current_chunk = test_chunk
                else:
                    if current_chunk:
                        result.append(current_chunk.strip())
                    current_chunk = sentence
            
            if current_chunk:
                result.append(current_chunk.strip())
            
            return result
        
        # Si no se puede dividir m√°s, truncar (√∫ltimo recurso)
        self.logger.warning(f"Texto no se puede dividir m√°s, truncando: {text[:100]}...")
        return [text[:max_size * 4]]  # Aproximaci√≥n de tokens a caracteres
    
    def chunk_document(self, text: str, metadata: Dict) -> List[Chunk]:
        """
        Dividir documento en chunks con fallback recursivo
        
        Args:
            text: Texto completo del documento
            metadata: Metadatos del documento
            
        Returns:
            Lista de chunks con metadatos
        """
        self.logger.info(f"Iniciando chunking de documento: {metadata.get('document_id', 'unknown')}")
        
        if not text.strip():
            self.logger.warning("Texto vac√≠o, retornando lista vac√≠a")
            return []
        
        # Intentar dividir por p√°rrafos primero
        paragraphs = self._split_by_paragraphs(text)
        chunks = []
        current_position = 1
        total_tokens = self._count_tokens(text)
        
        for paragraph in paragraphs:
            # Verificar si el p√°rrafo excede el tama√±o m√°ximo
            if self._count_tokens(paragraph) > self.chunk_size:
                # Aplicar fallback recursivo
                sub_chunks = self._apply_fallback_recursive(paragraph, self.chunk_size)
                
                for i, sub_chunk in enumerate(sub_chunks):
                    chunk = self._create_chunk_with_metadata(
                        text=sub_chunk,
                        position=current_position,
                        total_chunks=len(sub_chunks),  # Se actualizar√° al final
                        base_metadata=metadata,
                        start_token=0,  # Se calcular√° despu√©s
                        end_token=0
                    )
                    chunks.append(chunk)
                    current_position += 1
            else:
                # P√°rrafo normal, crear chunk
                chunk = self._create_chunk_with_metadata(
                    text=paragraph,
                    position=current_position,
                    total_chunks=1,  # Se actualizar√° al final
                    base_metadata=metadata,
                    start_token=0,
                    end_token=0
                )
                chunks.append(chunk)
                current_position += 1
        
        # Aplicar overlap entre chunks consecutivos
        chunks_with_overlap = self._apply_overlap(chunks)
        
        # Actualizar total_chunks y calcular posiciones de tokens
        total_chunks = len(chunks_with_overlap)
        for i, chunk in enumerate(chunks_with_overlap):
            chunk.total_chunks = total_chunks
            chunk.position = i + 1
            
            # Calcular posiciones de tokens (aproximado)
            chunk.start_token = i * self.chunk_size
            chunk.end_token = min((i + 1) * self.chunk_size, total_tokens)
        
        self.logger.info(f"Chunking completado: {len(chunks_with_overlap)} chunks creados")
        return chunks_with_overlap
    
    def _apply_overlap(self, chunks: List[Chunk]) -> List[Chunk]:
        """Aplicar overlap entre chunks consecutivos"""
        if len(chunks) <= 1:
            return chunks
        
        chunks_with_overlap = []
        
        for i, chunk in enumerate(chunks):
            if i > 0:
                # A√±adir overlap del chunk anterior
                prev_chunk = chunks[i - 1]
                overlap_text = prev_chunk.text[-self.overlap:] if len(prev_chunk.text) > self.overlap else prev_chunk.text
                
                # Combinar con el chunk actual
                combined_text = overlap_text + " " + chunk.text
                
                # Crear nuevo chunk con overlap
                new_chunk = self._create_chunk_with_metadata(
                    text=combined_text,
                    position=chunk.position,
                    total_chunks=chunk.total_chunks,
                    base_metadata=chunk.metadata,
                    start_token=chunk.start_token,
                    end_token=chunk.end_token,
                    overlap_start=len(overlap_text),
                    overlap_end=len(overlap_text) + len(chunk.text)
                )
                
                chunks_with_overlap.append(new_chunk)
            else:
                chunks_with_overlap.append(chunk)
        
        return chunks_with_overlap
    
    def validate_chunks(self, chunks: List[Chunk]) -> Dict[str, any]:
        """Validar que los chunks cumplen con los criterios"""
        validation_results = {
            'total_chunks': len(chunks),
            'chunks_within_size': 0,
            'chunks_with_overlap': 0,
            'errors': []
        }
        
        for i, chunk in enumerate(chunks):
            # Verificar tama√±o
            token_count = self._count_tokens(chunk.text)
            if token_count <= self.chunk_size:
                validation_results['chunks_within_size'] += 1
            else:
                validation_results['errors'].append(
                    f"Chunk {i+1} excede tama√±o m√°ximo: {token_count} > {self.chunk_size}"
                )
            
            # Verificar overlap
            if i > 0 and chunk.overlap_start is not None:
                validation_results['chunks_with_overlap'] += 1
        
        validation_results['success_rate'] = (
            validation_results['chunks_within_size'] / len(chunks) * 100
        )
        
        return validation_results
```

### 2. Crear Utilidades de Texto
Crear `src/utils/text_utils.py`:
```python
"""
Utilidades para procesamiento de texto legal
"""
import re
import unicodedata
from typing import List, Dict

def normalize_text(text: str) -> str:
    """Normalizar texto para b√∫squeda"""
    # Convertir a min√∫sculas
    text = text.lower()
    
    # Remover acentos
    text = ''.join(
        c for c in unicodedata.normalize('NFD', text)
        if not unicodedata.combining(c)
    )
    
    # Remover caracteres especiales
    text = re.sub(r'[^\w\s]', ' ', text)
    
    # Normalizar espacios
    text = re.sub(r'\s+', ' ', text).strip()
    
    return text

def extract_legal_entities(text: str) -> Dict[str, List[str]]:
    """Extraer entidades legales del texto"""
    entities = {
        'names': [],
        'dates': [],
        'amounts': [],
        'legal_terms': []
    }
    
    # Patrones para nombres (may√∫sculas seguidas de espacios)
    name_pattern = r'\b[A-Z√Å√â√ç√ì√ö√ë][A-Z√Å√â√ç√ì√ö√ë\s]+\b'
    names = re.findall(name_pattern, text)
    entities['names'] = [n.strip() for n in names if len(n.strip()) > 2]
    
    # Patrones para fechas
    date_patterns = [
        r'\d{1,2}/\d{1,2}/\d{4}',  # DD/MM/YYYY
        r'\d{4}-\d{1,2}-\d{1,2}',  # YYYY-MM-DD
        r'\d{1,2}\s+de\s+[a-z]+\s+de\s+\d{4}'  # DD de MES de YYYY
    ]
    
    for pattern in date_patterns:
        dates = re.findall(pattern, text)
        entities['dates'].extend(dates)
    
    # Patrones para cantidades monetarias
    amount_pattern = r'\$?\d{1,3}(?:\.\d{3})*(?:,\d{2})?'
    amounts = re.findall(amount_pattern, text)
    entities['amounts'] = amounts
    
    # T√©rminos jur√≠dicos comunes
    legal_terms = [
        'demandante', 'demandado', 'embargo', 'medida cautelar',
        'sentencia', 'recurso', 'apelaci√≥n', 'fundamento',
        'hechos', 'pruebas', 'testigo', 'abogado', 'juez'
    ]
    
    for term in legal_terms:
        if term in text.lower():
            entities['legal_terms'].append(term)
    
    return entities

def clean_text_for_chunking(text: str) -> str:
    """Limpiar texto para chunking"""
    # Remover caracteres de control
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', text)
    
    # Normalizar saltos de l√≠nea
    text = re.sub(r'\r\n', '\n', text)
    text = re.sub(r'\r', '\n', text)
    
    # Remover espacios m√∫ltiples
    text = re.sub(r' +', ' ', text)
    
    # Remover l√≠neas vac√≠as m√∫ltiples
    text = re.sub(r'\n\s*\n\s*\n', '\n\n', text)
    
    return text.strip()
```

### 3. Crear Tests Unitarios
Crear `tests/unit/test_chunking.py`:
```python
"""
Tests unitarios para el sistema de chunking
"""
import pytest
from src.chunking.document_chunker import DocumentChunker, Chunk
from src.utils.text_utils import normalize_text, extract_legal_entities, clean_text_for_chunking

class TestDocumentChunker:
    
    def setup_method(self):
        """Configurar antes de cada test"""
        self.chunker = DocumentChunker(chunk_size=100, overlap=20)
    
    def test_tokenize_text(self):
        """Test de tokenizaci√≥n de texto"""
        text = "Este es un texto de prueba con palabras."
        tokens = self.chunker._tokenize_text(text)
        
        expected = ['este', 'es', 'un', 'texto', 'de', 'prueba', 'con', 'palabras']
        assert tokens == expected
    
    def test_count_tokens(self):
        """Test de conteo de tokens"""
        text = "Este es un texto de prueba."
        count = self.chunker._count_tokens(text)
        assert count == 6  # este, es, un, texto, de, prueba
    
    def test_split_by_sentences(self):
        """Test de divisi√≥n por oraciones"""
        text = "Primera oraci√≥n. Segunda oraci√≥n! Tercera oraci√≥n?"
        sentences = self.chunker._split_by_sentences(text)
        
        assert len(sentences) == 3
        assert "Primera oraci√≥n" in sentences[0]
        assert "Segunda oraci√≥n" in sentences[1]
        assert "Tercera oraci√≥n" in sentences[2]
    
    def test_split_by_paragraphs(self):
        """Test de divisi√≥n por p√°rrafos"""
        text = "P√°rrafo uno.\n\nP√°rrafo dos.\n\nP√°rrafo tres."
        paragraphs = self.chunker._split_by_paragraphs(text)
        
        assert len(paragraphs) == 3
        assert "P√°rrafo uno" in paragraphs[0]
        assert "P√°rrafo dos" in paragraphs[1]
        assert "P√°rrafo tres" in paragraphs[2]
    
    def test_fallback_recursive(self):
        """Test de fallback recursivo"""
        # Texto que excede el tama√±o m√°ximo
        long_text = "Esta es una oraci√≥n muy larga " * 50
        
        chunks = self.chunker._apply_fallback_recursive(long_text, 50)
        
        assert len(chunks) > 1
        for chunk in chunks:
            assert self.chunker._count_tokens(chunk) <= 50
    
    def test_chunk_document_simple(self):
        """Test de chunking de documento simple"""
        text = "P√°rrafo uno. " * 10 + "\n\n" + "P√°rrafo dos. " * 10
        metadata = {'document_id': 'test_doc', 'demandante': 'Juan P√©rez'}
        
        chunks = self.chunker.chunk_document(text, metadata)
        
        assert len(chunks) > 0
        assert all(isinstance(chunk, Chunk) for chunk in chunks)
        assert all(chunk.metadata['document_id'] == 'test_doc' for chunk in chunks)
    
    def test_chunk_document_large(self):
        """Test de chunking de documento grande"""
        # Crear texto que exceda el tama√±o m√°ximo
        text = "Esta es una oraci√≥n de prueba. " * 100
        metadata = {'document_id': 'large_doc'}
        
        chunks = self.chunker.chunk_document(text, metadata)
        
        assert len(chunks) > 1
        for chunk in chunks:
            assert self.chunker._count_tokens(chunk.text) <= self.chunker.chunk_size
    
    def test_validate_chunks(self):
        """Test de validaci√≥n de chunks"""
        # Crear chunks de prueba
        chunks = [
            Chunk(
                id="test_1",
                text="Texto corto",
                position=1,
                total_chunks=2,
                metadata={'document_id': 'test'},
                start_token=0,
                end_token=10
            ),
            Chunk(
                id="test_2",
                text="Otro texto corto",
                position=2,
                total_chunks=2,
                metadata={'document_id': 'test'},
                start_token=10,
                end_token=20
            )
        ]
        
        validation = self.chunker.validate_chunks(chunks)
        
        assert validation['total_chunks'] == 2
        assert validation['chunks_within_size'] == 2
        assert validation['success_rate'] == 100.0

class TestTextUtils:
    
    def test_normalize_text(self):
        """Test de normalizaci√≥n de texto"""
        text = "N√öRY WILL√âLMA ROMERO G√ìMEZ"
        normalized = normalize_text(text)
        
        assert normalized == "nury willelma romero gomez"
        assert "√°" not in normalized
        assert "√©" not in normalized
    
    def test_extract_legal_entities(self):
        """Test de extracci√≥n de entidades legales"""
        text = "El demandante JUAN P√âREZ solicita embargo por $1,000,000 el 15/01/2024"
        entities = extract_legal_entities(text)
        
        assert "JUAN P√âREZ" in entities['names']
        assert "$1,000,000" in entities['amounts']
        assert "15/01/2024" in entities['dates']
        assert "demandante" in entities['legal_terms']
        assert "embargo" in entities['legal_terms']
    
    def test_clean_text_for_chunking(self):
        """Test de limpieza de texto para chunking"""
        text = "Texto  con   espacios   m√∫ltiples.\r\n\r\n\r\nL√≠neas vac√≠as."
        cleaned = clean_text_for_chunking(text)
        
        assert "   " not in cleaned
        assert "\r\n\r\n\r\n" not in cleaned
        assert "\n\n" in cleaned
```

### 4. Crear Script de Prueba
Crear `scripts/test_chunking.py`:
```python
#!/usr/bin/env python3
"""
Script para probar el sistema de chunking
"""
import sys
import os
import json
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.chunking.document_chunker import DocumentChunker
from src.utils.text_utils import clean_text_for_chunking
from config.settings import CSV_METADATA_PATH, JSON_DOCS_PATH
import pandas as pd

def main():
    print("üß™ Probando sistema de chunking...")
    
    # Crear chunker
    chunker = DocumentChunker()
    
    try:
        # Cargar metadatos
        df = pd.read_csv(CSV_METADATA_PATH)
        
        # Probar con el primer documento
        if len(df) > 0:
            doc_metadata = df.iloc[0].to_dict()
            
            # Cargar contenido
            json_path = os.path.join(JSON_DOCS_PATH, f"{doc_metadata['filename']}.json")
            
            if os.path.exists(json_path):
                with open(json_path, 'r', encoding='utf-8') as f:
                    content = json.load(f)
                    text = content.get('text', '')
                
                # Limpiar texto
                cleaned_text = clean_text_for_chunking(text)
                
                # Crear chunks
                chunks = chunker.chunk_document(cleaned_text, doc_metadata)
                
                # Validar chunks
                validation = chunker.validate_chunks(chunks)
                
                print(f"‚úÖ Chunking completado:")
                print(f"   - Documento: {doc_metadata['filename']}")
                print(f"   - Chunks creados: {len(chunks)}")
                print(f"   - Tasa de √©xito: {validation['success_rate']:.1f}%")
                print(f"   - Chunks con overlap: {validation['chunks_with_overlap']}")
                
                # Mostrar ejemplo de chunk
                if chunks:
                    example_chunk = chunks[0]
                    print(f"\nüìÑ Ejemplo de chunk:")
                    print(f"   - ID: {example_chunk.id}")
                    print(f"   - Posici√≥n: {example_chunk.position}/{example_chunk.total_chunks}")
                    print(f"   - Tokens: {example_chunk.metadata['token_count']}")
                    print(f"   - Texto: {example_chunk.text[:100]}...")
                
            else:
                print(f"‚ùå No se encontr√≥ el archivo JSON: {json_path}")
        else:
            print("‚ùå No hay documentos en el CSV")
            
    except Exception as e:
        print(f"‚ùå Error durante el testing: {e}")

if __name__ == "__main__":
    main()
```

## ‚úÖ Criterios de √âxito
- [ ] M√≥dulo `DocumentChunker` implementado correctamente
- [ ] Fallback recursivo funcionando para textos grandes
- [ ] Overlap aplicado entre chunks consecutivos
- [ ] Metadatos preservados en cada chunk
- [ ] Tests unitarios pasando
- [ ] Validaci√≥n de chunks funcionando
- [ ] Utilidades de texto implementadas

## üîç Verificaci√≥n
Ejecutar los siguientes comandos:
```bash
# Ejecutar tests
python -m pytest tests/unit/test_chunking.py -v

# Probar chunking
python scripts/test_chunking.py

# Verificar logs
cat logs/chunking.log
```

## üìä M√©tricas de Calidad
- **Tasa de √©xito**: > 95% de chunks dentro del tama√±o m√°ximo
- **Overlap**: Todos los chunks consecutivos deben tener overlap
- **Metadatos**: Todos los chunks deben preservar metadatos completos
- **Fallback**: Textos grandes deben dividirse sin p√©rdida de informaci√≥n

## üìù Notas Importantes
- El chunking debe preservar la estructura sem√°ntica de los documentos legales
- El fallback recursivo es cr√≠tico para documentos con p√°rrafos largos
- Los metadatos deben ser completos y consistentes
- La validaci√≥n debe ejecutarse despu√©s de cada chunking 